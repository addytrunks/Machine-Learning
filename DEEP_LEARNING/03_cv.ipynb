{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c776f7-8b54-48c7-b0e4-396edbf2147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "# functions for manipulating your vision data\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor # Converts PIL (python image lib) to tensor\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9a400e-54af-4ed0-9e39-ca41f6658051",
   "metadata": {},
   "source": [
    "## 1. Getting a Dataset\n",
    "We'll be using FashionMNIST as our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8d6fcfd-2afe-4373-a816-f45477b5be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up training data\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\", # where to download data to\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\", # where to download data to\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64d43ad8-cfea-4f13-aa24-3a0c26535177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n",
       "           0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0039, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n",
       "           0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n",
       "           0.0157, 0.0000, 0.0000, 0.0118],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n",
       "           0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0471, 0.0392, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n",
       "           0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n",
       "           0.3020, 0.5098, 0.2824, 0.0588],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n",
       "           0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n",
       "           0.5529, 0.3451, 0.6745, 0.2588],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n",
       "           0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n",
       "           0.4824, 0.7686, 0.8980, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n",
       "           0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n",
       "           0.8745, 0.9608, 0.6784, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n",
       "           0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n",
       "           0.8627, 0.9529, 0.7922, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n",
       "           0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n",
       "           0.8863, 0.7725, 0.8196, 0.2039],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n",
       "           0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n",
       "           0.9608, 0.4667, 0.6549, 0.2196],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n",
       "           0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n",
       "           0.8510, 0.8196, 0.3608, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n",
       "           0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n",
       "           0.8549, 1.0000, 0.3020, 0.0000],\n",
       "          [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n",
       "           0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n",
       "           0.8784, 0.9569, 0.6235, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n",
       "           0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n",
       "           0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n",
       "           0.9137, 0.9333, 0.8431, 0.0000],\n",
       "          [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n",
       "           0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n",
       "           0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n",
       "           0.8627, 0.9098, 0.9647, 0.0000],\n",
       "          [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n",
       "           0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n",
       "           0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n",
       "           0.8706, 0.8941, 0.8824, 0.0000],\n",
       "          [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n",
       "           0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n",
       "           0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n",
       "           0.8745, 0.8784, 0.8980, 0.1137],\n",
       "          [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n",
       "           0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n",
       "           0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n",
       "           0.8627, 0.8667, 0.9020, 0.2627],\n",
       "          [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n",
       "           0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n",
       "           0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n",
       "           0.7098, 0.8039, 0.8078, 0.4510],\n",
       "          [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n",
       "           0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n",
       "           0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n",
       "           0.6549, 0.6941, 0.8235, 0.3608],\n",
       "          [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n",
       "           0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n",
       "           0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n",
       "           0.7529, 0.8471, 0.6667, 0.0000],\n",
       "          [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n",
       "           0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n",
       "           0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n",
       "           0.3882, 0.2275, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n",
       "           0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "image,label = train_data[0]\n",
    "image,label\n",
    "# the label returns an integer 9, which corresponds to the 9th index of the classes (ankle boot, see below code cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d741d5c3-80c7-4763-9818-bf58d2776914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = train_data.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeea5a0-0878-4f07-94fe-10be0063b8b5",
   "metadata": {},
   "source": [
    "### 1.1 Check input and output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af5a52d7-b1b5-4fc8-9d7c-18310e6d679b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:torch.Size([1, 28, 28]) -> (Colour Channel, Height, Width) \n"
     ]
    }
   ],
   "source": [
    "print(\"Image shape:{} -> (Colour Channel, Height, Width) \".format(image.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0309d4fc-9739-4028-a0a4-b9f558026346",
   "metadata": {},
   "source": [
    "### 1.2 Visualise our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f682bf95-6c6b-47b8-ab65-f1858795b4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x245dc745f90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(class_names[label])\n",
    "plt.imshow(image.squeeze(),cmap='gray') # if squeeze isn't included, it'll give an error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598dd1ac-19cc-4d53-8acf-a02dab2f393a",
   "metadata": {},
   "source": [
    "## 2. Prepare Data Loader\n",
    "\n",
    "It turns our dataset into a Python iterables i.e turn it into mini batches.\n",
    "\n",
    "Why?\n",
    "1. Computationally efficient. So we break it down into batch size of 32.\n",
    "2. Gives our neural network to update its gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fff03d5-6f4d-44ec-88a1-feee17e258db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_data,batch_size=BATCH_SIZE,shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data,batch_size=BATCH_SIZE,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67c69d37-1cf6-446d-b910-1f6be4060b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_batch,train_labels_batch = next(iter(train_dataloader))\n",
    "train_features_batch[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa06ca35-30bc-48d1-86c0-6ccf5d2e73a0",
   "metadata": {},
   "source": [
    "## 3. Create a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "698b0e6c-1dfc-4566-8bdc-99b2f00a616a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before flattening: torch.Size([1, 28, 28]) -> [color_channels, height, width]\n",
      "Shape after flattening: torch.Size([1, 784]) -> [color_channels, height*width]\n"
     ]
    }
   ],
   "source": [
    "flatten_model = nn.Flatten()\n",
    "\n",
    "x = train_features_batch[0]\n",
    "\n",
    "output = flatten_model(x)\n",
    "\n",
    "print(f\"Shape before flattening: {x.shape} -> [color_channels, height, width]\")\n",
    "print(f\"Shape after flattening: {output.shape} -> [color_channels, height*width]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d486f81-3654-4626-979e-a14655deaac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTModelV0(nn.Module):\n",
    "    def __init__(self,input_shape:int,hidden_units:int,output_shape:int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=input_shape,out_features=hidden_units),\n",
    "            nn.Linear(in_features=hidden_units,out_features=output_shape),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94f1a05d-e0bf-4282-8c33-c2d7aa64f895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModelV0(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "model_0 = FashionMNISTModelV0(input_shape=784,hidden_units=10,output_shape=len(class_names))\n",
    "model_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82c0ce3-24ec-4dad-b76d-6cb13d544a23",
   "metadata": {},
   "source": [
    "### 3.1 Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35cff971-b538-45be-a2a9-a66a28a78f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy (a classification metric)\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
    "    acc = (correct / len(y_pred)) * 100 \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a192eaa-a55c-45b8-8292-09567506a3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helper_functions.py already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path \n",
    "\n",
    "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "  print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "  print(\"Downloading helper_functions.py\")\n",
    "  # Note: you need the \"raw\" GitHub URL for this to work\n",
    "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "  with open(\"helper_functions.py\", \"wb\") as f:\n",
    "    f.write(request.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab31e683-5388-45d8-bf71-56289469e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import accuracy metric\n",
    "from helper_functions import accuracy_fn # Note: could also use torchmetrics.Accuracy(task = 'multiclass', num_classes=len(class_names)).to(device)\n",
    "\n",
    "# Setup loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f9cb85c-7fa0-48c6-bdb5-d747d81f464a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer \n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    \"\"\"Prints difference between start and end time.\n",
    "\n",
    "    Args:\n",
    "        start (float): Start time of computation (preferred in timeit format). \n",
    "        end (float): End time of computation.\n",
    "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        float: time between start and end in seconds (higher is longer).\n",
    "    \"\"\"\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c58e0b-ddcd-4626-88c7-6cd18b08d455",
   "metadata": {},
   "source": [
    "### 3.2 Training and Test Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dac2dc-c045-4d19-a60f-491230a0b4ad",
   "metadata": {},
   "source": [
    "The model's parameters will be updated per batch rather than per sample.\n",
    "1. Loop through epochs.\r",
    "2. \n",
    "Loop through training batches, perform training steps, calculate the train loss per batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70ccd65f-5593-4d3e-8c2d-02ab713face5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "-------\n",
      "Looked at 0/60000 samples.\n",
      "Looked at 12800/60000 samples.\n",
      "Looked at 25600/60000 samples.\n",
      "Looked at 38400/60000 samples.\n",
      "Looked at 51200/60000 samples.\n",
      "Epoch: 1\n",
      "-------\n",
      "Looked at 0/60000 samples.\n",
      "Looked at 12800/60000 samples.\n",
      "Looked at 25600/60000 samples.\n",
      "Looked at 38400/60000 samples.\n",
      "Looked at 51200/60000 samples.\n",
      "Epoch: 2\n",
      "-------\n",
      "Looked at 0/60000 samples.\n",
      "Looked at 12800/60000 samples.\n",
      "Looked at 25600/60000 samples.\n",
      "Looked at 38400/60000 samples.\n",
      "Looked at 51200/60000 samples.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch: {epoch}\\n-------\")\n",
    "    ### Training\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch,(X,y) in enumerate(train_dataloader):\n",
    "        model_0.train()\n",
    "        \n",
    "        # 1. Forward Pass\n",
    "        y_pred = model_0(X)\n",
    "\n",
    "        # 2. Calculate Loss\n",
    "        loss = loss_fn(y_pred,y)\n",
    "        train_loss+=loss\n",
    "\n",
    "        # 3.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4.\n",
    "        loss.backward()\n",
    "\n",
    "        # 5.\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch%400 == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples.\")\n",
    "\n",
    "    train_loss/=len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2295750b-196f-4df2-b76e-3c3acf99eeca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'FashionMNISTModelV0',\n",
       " 'model_loss': 0.5108762383460999,\n",
       " 'model_acc': 82.37819488817891}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "def eval_model(model: torch.nn.Module, \n",
    "               data_loader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               accuracy_fn):\n",
    "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
    "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
    "        loss_fn (torch.nn.Module): The loss function of model.\n",
    "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
    "\n",
    "    Returns:\n",
    "        (dict): Results of model making predictions on data_loader.\n",
    "    \"\"\"\n",
    "    loss, acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # Make predictions with the model\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            # Accumulate the loss and accuracy values per batch\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true=y, \n",
    "                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -> pred_prob -> pred_labels)\n",
    "        \n",
    "        # Scale loss and acc to find the average loss/acc per batch\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "        \n",
    "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}\n",
    "\n",
    "# Calculate model 0 results on test dataset\n",
    "model_0_results = eval_model(model=model_0, data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn, accuracy_fn=accuracy_fn\n",
    ")\n",
    "model_0_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9922f9af-9a1c-4f28-b3b4-aa4740f146b5",
   "metadata": {},
   "source": [
    "## 4. Make Predictions with Model 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72638c5f-774c-435f-a18d-b1053103548e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'FashionMNISTModelV0',\n",
       " 'model_loss': 0.5108762383460999,\n",
       " 'model_acc': 82.37819488817891}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "def eval_model(model: torch.nn.Module, \n",
    "               data_loader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               accuracy_fn):\n",
    "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
    "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
    "        loss_fn (torch.nn.Module): The loss function of model.\n",
    "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
    "\n",
    "    Returns:\n",
    "        (dict): Results of model making predictions on data_loader.\n",
    "    \"\"\"\n",
    "    loss, acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # Make predictions with the model\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            # Accumulate the loss and accuracy values per batch\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true=y, \n",
    "                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -> pred_prob -> pred_labels)\n",
    "        \n",
    "        # Scale loss and acc to find the average loss/acc per batch\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "        \n",
    "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}\n",
    "\n",
    "# Calculate model 0 results on test dataset\n",
    "model_0_results = eval_model(model=model_0, data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn, accuracy_fn=accuracy_fn\n",
    ")\n",
    "model_0_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745daba6-4215-4bc8-880f-4a7ca4aadd0a",
   "metadata": {},
   "source": [
    "## 5. Device Agnostic + Non-Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e119216c-a557-440e-85f8-a075122589ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup device agnostic code\n",
    "import torch\n",
    "device = \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6c2356f-1612-4425-9553-22b97f60fa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model with non-linear and linear layers\n",
    "class FashionMNISTModelV1(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(), # flatten inputs into single vector\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e240402-09b5-4d51-a28d-fae564230996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "model_1 = FashionMNISTModelV1(input_shape=784, # number of input features\n",
    "    hidden_units=10,\n",
    "    output_shape=len(class_names) # number of output classes desired\n",
    ").to(device) # send model to GPU if it's available\n",
    "next(model_1.parameters()).device # check model device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe14667a-a4fc-456c-8d8f-14293ca65750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import accuracy_fn\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_1.parameters(), \n",
    "                            lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0af3408-8c36-4804-ba1d-0ca902763756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn,\n",
    "               device: torch.device = device):\n",
    "    train_loss, train_acc = 0, 0\n",
    "    model.to(device)\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        # Send data to GPU\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "        train_acc += accuracy_fn(y_true=y,\n",
    "                                 y_pred=y_pred.argmax(dim=1)) # Go from logits -> pred labels\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate loss and accuracy per epoch and print out what's happening\n",
    "    train_loss /= len(data_loader)\n",
    "    train_acc /= len(data_loader)\n",
    "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62b2a545-f7e0-45cb-ac56-20ba6398c2cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4109cc4abb842f1bdda27e7213bce55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "---------\n",
      "Train loss: 1.09199 | Train accuracy: 61.34%\n",
      "Epoch: 1\n",
      "---------\n",
      "Train loss: 0.76867 | Train accuracy: 72.45%\n",
      "Epoch: 2\n",
      "---------\n",
      "Train loss: 0.66650 | Train accuracy: 76.09%\n",
      "Train time on cuda: 47.590 seconds\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Measure time\n",
    "from timeit import default_timer as timer\n",
    "train_time_start_on_gpu = timer()\n",
    "\n",
    "epochs = 3\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n---------\")\n",
    "    train_step(data_loader=train_dataloader, \n",
    "        model=model_1, \n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        accuracy_fn=accuracy_fn\n",
    "    )\n",
    "train_time_end_on_gpu = timer()\n",
    "total_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,\n",
    "                                            end=train_time_end_on_gpu,\n",
    "                                            device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8188e8-f9c4-4f9a-94df-3448c3c64a88",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7df149-9bd2-4fc1-8b71-ce03de74e417",
   "metadata": {},
   "source": [
    "# MODEL 2 : CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c7de73e-4d51-455b-9fad-9d458217f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTV2(nn.Module):\n",
    "    def __init__(self,input_shape:int,hidden_units:int,output_shape:int):\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape,out_channels=hidden_units,kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,out_channels=hidden_units,kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_units,out_channels=hidden_units,kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,out_channels=hidden_units,kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=hidden_units*0,out_features=output_shape)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.conv_block_1(x)\n",
    "        print(x.shape)\n",
    "        x = self.conv_block_2(x)\n",
    "        print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c042989b-b78a-49ae-aeb0-2c680db282fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model2 = FashionMNISTV2(input_shape=1,output_shape=len(class_names),hidden_units=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "457123a5-e977-46e8-b6f8-f1854261acb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import accuracy_fn\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model2.parameters(), \n",
    "                            lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32956a46-741f-4dc5-91a2-135f9d6295ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f4a33c5a6340f09ca2014200717bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "---------\n",
      "torch.Size([32, 10, 14, 14])\n",
      "torch.Size([32, 10, 7, 7])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "only batches of spatial targets supported (3D tensors) but got targets of dimension: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m---------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m     train_step(data_loader\u001b[38;5;241m=\u001b[39mtrain_dataloader, \n\u001b[0;32m     12\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel2, \n\u001b[0;32m     13\u001b[0m         loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[0;32m     14\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m     15\u001b[0m         accuracy_fn\u001b[38;5;241m=\u001b[39maccuracy_fn,\n\u001b[0;32m     16\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[0;32m     17\u001b[0m     )\n\u001b[0;32m     19\u001b[0m train_time_end_model_2 \u001b[38;5;241m=\u001b[39m timer()\n\u001b[0;32m     20\u001b[0m total_train_time_model_2 \u001b[38;5;241m=\u001b[39m print_train_time(start\u001b[38;5;241m=\u001b[39mtrain_time_start_model_2,\n\u001b[0;32m     21\u001b[0m                                            end\u001b[38;5;241m=\u001b[39mtrain_time_end_model_2,\n\u001b[0;32m     22\u001b[0m                                            device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[1;32mIn[23], line 17\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, data_loader, loss_fn, optimizer, accuracy_fn, device)\u001b[0m\n\u001b[0;32m     14\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 2. Calculate loss\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y)\n\u001b[0;32m     18\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m     19\u001b[0m train_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m accuracy_fn(y_true\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m     20\u001b[0m                          y_pred\u001b[38;5;241m=\u001b[39my_pred\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m# Go from logits -> pred labels\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1175\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m   1176\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: only batches of spatial targets supported (3D tensors) but got targets of dimension: 1"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Measure time\n",
    "from timeit import default_timer as timer\n",
    "train_time_start_model_2 = timer()\n",
    "\n",
    "# Train and test model \n",
    "epochs = 3\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n---------\")\n",
    "    train_step(data_loader=train_dataloader, \n",
    "        model=model2, \n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        accuracy_fn=accuracy_fn,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "train_time_end_model_2 = timer()\n",
    "total_train_time_model_2 = print_train_time(start=train_time_start_model_2,\n",
    "                                           end=train_time_end_model_2,\n",
    "                                           device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5efe1489-c52d-4bb8-8ff2-0164ef2f5053",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Get model_2 results \u001b[39;00m\n\u001b[0;32m      2\u001b[0m model_2_results \u001b[38;5;241m=\u001b[39m eval_model(\n\u001b[1;32m----> 3\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_2,\n\u001b[0;32m      4\u001b[0m     data_loader\u001b[38;5;241m=\u001b[39mtest_dataloader,\n\u001b[0;32m      5\u001b[0m     loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[0;32m      6\u001b[0m     accuracy_fn\u001b[38;5;241m=\u001b[39maccuracy_fn\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m model_2_results\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_2' is not defined"
     ]
    }
   ],
   "source": [
    "# Get model_2 results \n",
    "model_2_results = eval_model(\n",
    "    model=model_2,\n",
    "    data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn,\n",
    "    accuracy_fn=accuracy_fn\n",
    ")\n",
    "model_2_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318e5fc1-ac04-48d2-87ce-0c67ee146185",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
